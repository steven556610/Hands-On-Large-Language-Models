{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adFzzFsB-Ofl"
   },
   "source": [
    "<h1>Chapter 3 - Looking Inside Transformer LLMs</h1>\n",
    "<i>An extensive look into the transformer architecture of generative LLMs</i>\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
    "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is for Chapter 3 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install transformers>=4.41.2 accelerate>=0.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_23Z_do-faF"
   },
   "source": [
    "# Loading the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759,
     "referenced_widgets": [
      "5ecafba0f8f04685a56d2b1495baea24",
      "53945d03a26044878ddb7fc6eadfd8db",
      "99db408011ea43c79adbd4a880839484",
      "fd181746067945febc55f94e2dcf6f67",
      "c5acd3e22bbe4a1897f7a12051e8eae9",
      "446d084fde5f422a8c0525e8c5b47f93",
      "6a0dabe02c874ecabd257c5da5f4a7c5",
      "1ee8996717864f79af6cf0314cd27c59",
      "7c719ce3694845b384ad4ae7207d31cc",
      "989fc06dfb25420eaf155bfc0174c692",
      "443850bc37d94a90aa037e73a77f9369",
      "184bbd6daf424ddeba7f8cbe0b5b34d9",
      "a6ee07658a234a54b38d849ff2017d6c",
      "bf63e0a23fa147778dd88b48075611e3",
      "acf8b6868c484885871964cc538359c6",
      "32cc3b668de84e8a9f8cde25d3846822",
      "b1115e5097c4404694e5a7c228e25234",
      "6ca20198bc144510a14b795aaf813940",
      "982a35743f51448cba583e54e7d3987d",
      "17250e2791ba42099c50efa594e229ce",
      "a4c438b7029f47d9a54d5a0ce16541c0",
      "fafd3d7714d4466ea00f354b20dc954a",
      "0a18c83d2645496797d74aef5e84dafa",
      "bfbbea9912104cb1803c7c827c1cea7d",
      "18c23f29824e4cb58e461bf89811a32a",
      "74105769e6a541098769921b84d1f1cd",
      "52109f76852e4388b6df4e5441c43a48",
      "4f39386aea7c4ac9abf2d66291cbcb4e",
      "4893546c2d6f4eec9cb5b7f6853013f0",
      "88616761593a4b2b9365294427a3136d",
      "d3ab3ad192f54d5b9f9e08db62884f0c",
      "6be54d3b31864134bc36d3b9d997530a",
      "27a1df02460948e19cbd40b69a89bec6",
      "6b81bad9c639454980de2a67b414b988",
      "1e8224a73a724058b28a13db2c2197c2",
      "3cbbd1ae4e4d4bf1a34c9d960ee7d26d",
      "e9c0309e4af44726b180c977692e2469",
      "97421a19ce25438e846a84b45321f9d0",
      "c6aa0ad56ce44b1eab1356d3bc56706b",
      "87d0758fdaaa4d90b21b1291f4d20039",
      "8c85660c9e9f4027bd64d596160a7d7f",
      "fce8b8595211452f8a759a6e98410f6a",
      "4249df3055d4427493e2a2e775dc1a93",
      "e42e39002e66410b9d714656a632adea",
      "3cf79ac9541c4b7bbf686b604ff73b81",
      "b709537463c3485dbbcef93e3636af2d",
      "cf7c24d7629b4362ae0905f8e4bbd997",
      "d9f9f98cb9dd465daed1da06d7d4084b",
      "cb20a4ccd9ea4548bc7f694068bb30f6",
      "b91d534a7a5c4863a9e858cedbda9fd5",
      "a65601c4918c4c2cb2020837cd1e1f85",
      "99505d538b894a359bc3791cd95423d5",
      "1a48245eddfc4923a5f267cd799ba9c7",
      "6c50ddb227b4421da5cd391e4d6ec94c",
      "7d62b5c9c90644378b0ca96cee430419",
      "9ec56d025f8446a08184b055fe11598e",
      "54894d44acce4fe6b0ef749d5c02e3cd",
      "e2dd956536a0407fb9b9c4a01c01ba9c",
      "54370883565644c5a2529e092db7f259",
      "7ec24f1bff5f4ca78d7c36a637cfa294",
      "36ff492b5c7c4ff3bcf72cc574d03a38",
      "8279cee867884166bb09df4e02635e2a",
      "a2644dee82b14cfbab069a484f3841e2",
      "e761a9afe1b847579fb51eb0eddd4488",
      "c59d9e04e5964a8891cc3ddce33d6f86",
      "a9007e7552ab4634ac44577279b242ac",
      "2cff15834149418c81eee5239a5e275a",
      "6623f077d95f4faf883fa2ca4397169d",
      "98529c941229460da8563b94d3419c37",
      "3bef1bf002c94340a3323592d616e7ac",
      "47c8fd6b7c9844d983014e07f1999cd2",
      "aa24ef9c6e1d49708b7bb4755a9adb78",
      "28261270cd0449f8ac85dc4b0efdac57",
      "a231962de8584f69a6a107c275d1cda5",
      "2d35dfe75a744987ab210f5fb0118301",
      "1cbaa3bcf20b4af099f6d4310dd071dd",
      "4f7088db853e47e6b6a6ccd654e379dc",
      "7e0710dc5b5c4002b4bcc189bf5514cf",
      "66dd13ca8234409eab15ecbf7a009914",
      "8f021e2bfb7246248baa49b08f4d3358",
      "0a91788829fc49ef95a69efd3256a8e3",
      "b42a56f9b2e44b6992413e023ed44b0e",
      "f9fc98e3d8ed4338bd763d152f8cc5f9",
      "f786e0117562476697092ef828ceb1b2",
      "07fed042c0894ca5aebe717eca6f3018",
      "3bea08fef3ef456e9f180d8a23ded5bc",
      "35f3727f37c44640986d8416141f8069",
      "0dd1ce8a2306431c9b0412e5992f4f84",
      "dcea31213c9f421abc7ffabc3499ecb9",
      "61643fa2fcb54ebaad8da5106dec9ea0",
      "0c4f8c58d213493494120c070d86ac76",
      "e15e9978db794891b9fa0d8ce096c983",
      "b15098d69b5f418da3f81aba8fb79de0",
      "f7cf40042a2e4c8cb8b87444893d8ec1",
      "6dcbc6ec46d44a55a1b38267201926b8",
      "6de4dd6e36b444638b2f65e0ca80bc9a",
      "60febb04a12447c192e1b8eb2aa5ba28",
      "6d200e6b1bdc4e918093670df8e37dc8",
      "934e950a3c0c48f08802d551ee1bd429",
      "34058533e3cf46a88db8927372102b9f",
      "2a7ba0f87814436386e66f4ef7f1111d",
      "d3e41286c5b747a8bb5cf326f9f80ad3",
      "fe92dc2d5e8c449bbb36dafbd6c9935f",
      "615d4ec0fb194688a392b71b99cf3621",
      "e2056e3dba884242807a98b9b3837843",
      "6ca50fb814b64fccaf0e1c6c11d8f4d8",
      "9ec1d173921748b2af19a3a21df9ed40",
      "f8c3d566e98d47239ef2b823544b75a5",
      "3618dcec35a740c485ecafa5589e0c91",
      "f66dd730f3364e35974a3918d12ff51d",
      "dcb9240335394bfa8d3949ef1cdbcdf8",
      "2df88b2fd5e242eca8b1d3f6cea1349b",
      "1b103c69baf74fceb551ebcb5a0ac5e8",
      "6847b7b6b3854d6e9b72f40040f84c8f",
      "ac3e67f03f604883ab4787930cd316f3",
      "6d7e012a7fbf4d788054ead5020e9314",
      "a432e2e32c7c4e56b138ebaefad76c93",
      "352fe4ea215240149d73478e34cd9b66",
      "bf9bc31d7f99477bb391f69df41b8dbe",
      "41b8e463309e4013a50268677f44d4b5",
      "989326b74cd146e3b5b2f2d5f19bdf41",
      "36b7269d8eb849b084694fd1f3b177b9",
      "2d09f9ed15fa452ba8d9ce9aba9f61ac",
      "53818082d62749a28552a1eebd304d88",
      "766f67cb6a8a4a58965edb671ce624e8",
      "a86d22308dbf4c11bf3d6f6515aef561",
      "e9e7d944715b402ab149f86862b92259",
      "4aaa732bb1b94b4895ca3f00f93cd762",
      "9fb02b3bbe79434a93f32291c208aaad",
      "dd9b3a5e84ba44cb9717ded470c258b4",
      "fa89524d446b480aa50d203d01ec7bb7",
      "c8b77256d5fc436fbfdcc150843a6b5b",
      "f5f5b592768048169676e09cca453645",
      "77f40b8bf30c437ba987b71178d0e9f6",
      "3f39ec300bd84852a2388dadaafd8c4b",
      "250cac43e6da47dd8d732ea57d8c50ec",
      "f929d12aad68458b98c21e0669da3d8e",
      "44ddbcadcc4c477c80daf278122de46d",
      "eb606db4125e4eb097d5b7d3cdb90976",
      "b6c177de60b54edd887d1ca983ea7546",
      "6db56d7c52244a3984a0638e060a81cb",
      "11d17ae63dd44ecc8813d482ee17dd95",
      "ff3733c6a1f34580b037e296e3abed6b",
      "2d31d51641e945f695f7315b68e0ad2e",
      "1b694930328e46bd9e0d61063b9141d4",
      "b544b6f2c2bb4f36bf9a983005a8bdb8",
      "afb8b0e602b649fcb92634c8aab4caf7",
      "24fc703c916f43aab5900288b8aa5aca",
      "16cffa93ab234718a8ede1044596e8b2",
      "2d7999217424413d988cd29d41ed5ace",
      "0d1546581c90418fa1cfc37491339134",
      "0e233853a3b74211a0b65dcdd001feed",
      "a1d2163af40a4aaab8a765b148845807",
      "2aaa722b303b4e1f823cee828fc7958c"
     ]
    },
    "executionInfo": {
     "elapsed": 130259,
     "status": "ok",
     "timestamp": 1718959891215,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "-5RLd6dI-Ytm",
    "outputId": "fb085ff7-e06f-4142-8e95-5ff98b212e37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d13b08e30147a6a253bab251f09395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REqcz-ID_XgV"
   },
   "source": [
    "# The Inputs and Outputs of a Trained Transformer LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4955,
     "status": "ok",
     "timestamp": 1718959896168,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "17h6TPHluJ-i",
    "outputId": "18727eeb-ccd6-40f8-aab1-25c8d9a03cbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mention the steps you're taking to prevent it in the future.\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \" Mention the steps you're taking to prevent it in the future.\\n\\nDear Sarah,\\n\\nI hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1718959898745,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "eoFkdTd6_g5o",
    "outputId": "bdcfde9f-28b7-4f43-ec0c-32c16677a776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3ForCausalLM(\n",
      "  (model): Phi3Model(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTrwzB67BYVY"
   },
   "source": [
    "# Choosing a single token from the probability distribution (sampling / decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步驟 2 的輸出：model_output[0] (隱藏狀態)\n",
    "當您執行 model_output = model.model(input_ids) 時，model.model（或 Phi3Model）完成了所有 32 個 Transformer 層的計算。\n",
    "\n",
    "輸出性質： model_output[0] 是一個 3D Tensor，形狀是 (Batch Size, Sequence Length, Hidden Size)。\n",
    "\n",
    "這 3072 維的 Hidden Size 向量，就是模型對每個輸入詞元及其上下文的最終數學理解。\n",
    "\n",
    "它不是機率，也不是詞元 ID，它是高維度的、語義豐富的數字向量**。**\n",
    "\n",
    "步驟 3 的目的：lm_head_output (Logits)\n",
    "lm_head 的定義是：Linear(in_features=3072, out_features=32064, bias=False)。\n",
    "\n",
    "LM Head 的任務： 它的唯一任務就是充當翻譯器或投影儀。\n",
    "\n",
    "它接收 3072 維的隱藏狀態向量（模型對詞彙的內部理解）。\n",
    "\n",
    "它將這個向量投影到 32064 維的詞彙表空間上。\n",
    "\n",
    "輸出性質： lm_head_output 就是 Logits。\n",
    "\n",
    "它是一個 3D Tensor，形狀是 (Batch Size, Sequence Length, Vocabulary Size)。\n",
    "\n",
    "在 Vocabulary Size（32064）這個維度上，每個數值代表模型預測下一個詞元是詞彙表中對應的詞元的分數（或 Logit）。\n",
    "\n",
    "注意： Logits 尚未經過 softmax 轉換為標準機率。\n",
    "\n",
    "因此，lm_head 必須接在 model.model 的輸出（隱藏狀態向量）之後，才能將模型的語義特徵轉換為預測機率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sEcxYgJxBYbJ"
   },
   "outputs": [],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = input_ids.to(\"cpu\")\n",
    "\n",
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)\n",
    "\n",
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取Logits 陣列\n",
    "lm_head_output[0,-1]\n",
    "\n",
    "# 找到最高分預測\n",
    "...argmax(-1)\n",
    "argmax 是為了找到最大的索引值\n",
    "使用-1 是為了在最後一個維度上進行搜尋，即是Vocabulary size，長度為32064。\n",
    "\n",
    "# 結果： \n",
    "lm_head_output[0, -1] 是一個 32064 維的向量（每個詞彙的分數）。執行 argmax(-1) 會返回這個 32064 個分數中，分數最高的那個詞彙所對應的 ID 數字。\n",
    "\n",
    "這個結果，token_id，就是模型在給定輸入 \"The capital of France is\" 之後，最確信應該接上的下一個詞元 ID。\n",
    "\n",
    "\n",
    "總結\n",
    "這兩行程式碼的作用是：\n",
    "\n",
    "專注於輸入序列末尾的預測分數。\n",
    "\n",
    "確定分數最高的下一個詞元 ID。\n",
    "\n",
    "將該 ID 翻譯 成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1718960391623,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "68YUSS4GBf9Q",
    "outputId": "2dc25e8d-03b6-4bca-b46c-fec3e3a4a492"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1) #可以得到，拿所有的token 經過模型後，預測出來的下一個詞對應出來的機率，只是output還有經過lm_head進行對應。對映完還要decode\n",
    "tokenizer.decode(token_id)\n",
    "# lm_head_output 會是五個詞元的 1,5,32064。\n",
    "# 但建立於casualLM的機制，我只需要提供其中最後一個的32064，就已經是下一個詞元的機率。\n",
    "# 因為 這一個結果是，模型對整句話進行的理解。“The capital of France is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1718960415287,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "cWWrfC5oBjwp",
    "outputId": "c2fdeab7-e787-466f-88f4-988cd5f939a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3072])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1718960424560,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "nC1PdOnTBnxZ",
    "outputId": "1fd5f482-7046-4536-b745-4e681d6ecdaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 32064])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2_rP4QBqrZ"
   },
   "source": [
    "# Speeding up generation by caching keys and values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|特性|use_cache=True (預設值)|use_cache=False|\n",
    "|:----:|:----:|:----:|\n",
    "|功能|啟用 KV Cache (Key/Value Cache) 機制。|關閉 KV Cache 機制。|\n",
    "|計算方式|儲存之前計算的 K (Key) 和 V (Value) 向量。在生成新 Token 時，只計算新 Token 的 K/V 並將其連接到 Cache。|每個生成步驟中，重新計算整個序列的 K 和 V 向量。|\n",
    "|推理速度|快得多。計算量隨著序列長度線性增加（O(1)）。|慢得多。計算量隨著序列長度平方增加（O(L square)）。|\n",
    "|記憶體用量|高。需要額外記憶體來儲存 Cache（K 和 V 向量）。|低。不儲存 Cache，但每次計算都需要大量臨時記憶體。|\n",
    "|結果|通常相同 (預期)。但在使用低精度（如 bfloat16）時，可能因浮點誤差累積導致微小差異。|通常相同。|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0n6JhNHBrin"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tokenize the input prompt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m----> 5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thellmbook/lib/python3.10/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47155,
     "status": "ok",
     "timestamp": 1718960517928,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "BwIvt6jSByAF",
    "outputId": "e71c4141-2ca3-488a-fdfb-8d9357af0125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.66 s ± 2.22 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=100,\n",
    "  use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152674,
     "status": "ok",
     "timestamp": 1718960670601,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "dFb1dcvJByCW",
    "outputId": "0aba6a01-9bc7-40b7-e2e1-e064f13b4c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2h 44min 35s ± 23min 30s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=100,\n",
    "  use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
